### requests请求

```python
import requests
url = "http://www.qq.com/"
resp = requests.get(url)
resp.status_code
resp.text
```

### BeautifulSoup解析网页

```python
from bs4 import BeautifulSoup
soup = BeautifulSoup(resp.text,"lxml")
soup # 树形网页结构
soup.select("a") #找出全部标签a，即链接
for item in soup.select("a"):
    print(item.text) # 打印标签文本
    print(item["href"]) # 打印属性值
   
soup.select("#newsContent01") #取出指定ID的标签
soup.select(".navBetaInner")
[item.text for item in soup.select(".navBetaInner")] #没有深一层区分开来
[item.text for item in soup.select(".navBetaInner a")] #深入一层区分开来
```



为了防止被禁，将爬虫伪装得更像浏览器：

添加请求的header字段

```python
headers = {
    'Accept':'application/json, text/javascript, */*; q=0.01',
    'Accept‐Encoding':'gzip, deflate, br',
    'Accept‐Language':'zh‐CN,zh;q=0.8',
    'Connection':'keep‐alive',
    'Content‐Length':'64',
    'Content‐Type':'application/x‐www‐form‐urlencoded; charset=UTF‐8'
}
s = requests.Session()
s.headers = headers
```

还可以添加超时：

```python
resp = s.get(url,timeout=15)
```

爬取拉钩网的一个例子：

```python
def get_one_page(city,page):
    data = {
        "first":"true",
        "pn":page,
        "kd":job_title
    }
    url = "https://www.lagou.com/jobs/positionAjax.json?
px=default&city=%s&needAddtionalResult=false"%city
    try:
        try:
            resp = s.post(url,data=data,timeout=20)
        except:
            try:
                resp = s.post(url,data=data,timeout=20)
            except:
                resp = s.post(url,data=data,timeout=20)
        json_Data = json.loads(resp.text)
        return json_Data
    except Exception as e:
        print("访问城市：%s的第%d页列表页失败！出错原因：%s"%(city,page,e))
        return None
```

###  正则化

```python
res = soup.find('ul', {'class':'jan'})
res = soup.find_all('li')
res = soup.find_all('li',{'src':re.compile('***')})
```

### 多进程分布式爬虫和IP代理池

通过多进程来可以实现分布式爬虫，速度比普通爬虫要快，进程数可以自己指定，一般指定为CPU核数。

一般直接爬虫容易被对方网站限制（禁止IP），可以使用IP代理来防止被禁。

下面是用多进程实现的IP代理池示例：

代理IP使用的是[高匿](http://www.xicidaili.com/nn/)IP

```python
#导入相关包
import requests as rq
from bs4 import BeautifulSoup as bs
import time
import random
import multiprocessing as mp
from functools import reduce

#从代理IP网站上分别处理每页的函数，输入是每页的url地址
def getIPProxy(url):
    #添加headers伪造浏览器
    headers = {'User-Agent':'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Mobile Safari/537.36'}
    #这里可以使用proxies = {'proxy':'http://114.99.13.171:21686'}，也可以不用，此IP是手动从高匿网站上复制下来的，目前可能不能用，如果不能用直接将proxies = {'proxy':'http://114.99.13.171:21686'}去掉或者更换IP和端口
    soup = bs(rq.get(url, headers = headers, timeout = 20,proxies = {'proxy':'http://114.99.13.171:21686'}).text, 'lxml')
    #设置随机等待防止被禁（可去掉）
    time.sleep(random.random()*5)
    ip_hosts = soup.select('#ip_list tr')
    ips = [item.select('td')[1].text + ':' + item.select('td')[2].text for item in ip_hosts[1:]]
    #返回当前页上所有代理IP
    return ips

#定义函数检测每个代理IP是否可用，输入是一个代理IP，形式为：http://***.***.***.***.:port
def testIP(ip):
	headers = {'User-Agent':'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Mobile Safari/537.36'}
	url = 'http://www.baidu.com/'
	status = 'ok'
	try:
		resp = rq.get(url, proxies = {'proxy':ip}, headers = headers, timeout = 20)
		print(resp)
	except:
		status = 'no'
	if status is 'ok':
		return ip
	else:
		return None
 
#主函数，创建多进程，获取每页上的代理IP并且测试，最终返回可用的所有代理IP，输入时页数，也就是爬取网站多少页数
def getProxyPool(pages):
    #创建进程，一般创建个数跟CPU核数相同使其充分利用CPU，不是越多越好，太多的话进程间切换可能会增加消耗
    pool = mp.Pool(processes = 4)
    pageurls = ['http://www.xicidaili.com/nn/' + str(i) for i in range(1,pages)]
    proxyPools = pool.map(getIPProxy, pageurls)
    proxyPool = list(map(lambda x : r'http://'+x,reduce(lambda x, y : x+y,proxyPools)))
    result = pool.map(testIP, proxyPool)
    ippool = list(filter(lambda elem : elem != None, result))
    pool.close()
    pool.join()
    print('----------IP Proxy Pool Finished----------')
    return ippool


#获取IP代理，可以存储起来
if __name__ == '__main__':
	proxies = getProxyPool(2)
	with open(r'C:\Users\chengsong\Desktop\proxies.txt', 'w') as writer:
		for proxy in proxies:
			writer.write(proxy + '\n')
```


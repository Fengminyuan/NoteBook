### requests请求

```python
import requests
url = "http://www.qq.com/"
resp = requests.get(url)
resp.status_code
resp.text
```

### BeautifulSoup解析网页

```python
from bs4 import BeautifulSoup
soup = BeautifulSoup(resp.text,"lxml")
soup # 树形网页结构
soup.select("a") #找出全部标签a，即链接
for item in soup.select("a"):
    print(item.text) # 打印标签文本
    print(item["href"]) # 打印属性值
   
soup.select("#newsContent01") #取出指定ID的标签
soup.select(".navBetaInner")
[item.text for item in soup.select(".navBetaInner")] #没有深一层区分开来
[item.text for item in soup.select(".navBetaInner a")] #深入一层区分开来
```



为了防止被禁，将爬虫伪装得更像浏览器：

添加请求的header字段

```python
headers = {
    'Accept':'application/json, text/javascript, */*; q=0.01',
    'Accept‐Encoding':'gzip, deflate, br',
    'Accept‐Language':'zh‐CN,zh;q=0.8',
    'Connection':'keep‐alive',
    'Content‐Length':'64',
    'Content‐Type':'application/x‐www‐form‐urlencoded; charset=UTF‐8'
}
s = requests.Session()
s.headers = headers
```

还可以添加超时：

```python
resp = s.get(url,timeout=15)
```

爬取拉钩网的一个例子：

```python
def get_one_page(city,page):
    data = {
        "first":"true",
        "pn":page,
        "kd":job_title
    }
    url = "https://www.lagou.com/jobs/positionAjax.json?
px=default&city=%s&needAddtionalResult=false"%city
    try:
        try:
            resp = s.post(url,data=data,timeout=20)
        except:
            try:
                resp = s.post(url,data=data,timeout=20)
            except:
                resp = s.post(url,data=data,timeout=20)
        json_Data = json.loads(resp.text)
        return json_Data
    except Exception as e:
        print("访问城市：%s的第%d页列表页失败！出错原因：%s"%(city,page,e))
        return None
```

###  正则化

```python
res = soup.find('ul', {'class':'jan'})
res = soup.find_all('li')
res = soup.find_all('li',{'src':re.compile('***')})
```

